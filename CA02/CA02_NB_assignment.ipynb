{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCZYXwtCsL_y"
   },
   "source": [
    "CA02: This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm. \n",
    "\n",
    "In this assignment you will ...\n",
    "1. Complete the code such a way that it works correctly with this given parts of the program.\n",
    "2. Explain as clearly as possible what each part of the code is doing. Use \"Markdown\" texts and code commenting to explain the code\n",
    "\n",
    "IMPORTANT NOTE:\n",
    "\n",
    "The path of your data folders 'train-mails' and 'test-mails' must be './train-mails' and './test-mails'. This means you must have your .ipynb file and these folders in the SAME FOLDER in your laptop or Google Drive. The reason for doing this is, this way the peer reviewes and I would be able to run your code from our computers using this exact same relative path, irrespective of our folder hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.0-1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Users/vishwamshankar/anaconda3/envs/BSAN-6088-capsprep/lib/python3.11/site-packages (from scikit-learn) (1.26.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.12.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (165 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.4/165.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.4.0-1-cp311-cp311-macosx_12_0_arm64.whl (10.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp311-cp311-macosx_12_0_arm64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Installing the scikit-learn library, which includes the Gaussian Naive Bayes algorithm and accuracy metrics\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p_DvtT7sOIr",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os # for interacting with the operating system\n",
    "import numpy as np # for numerical operations\n",
    "from collections import Counter # to count occurrences of each word\n",
    "\n",
    "# Importing Gaussian Naive Bayes and accuracy score calculation from scikit-learn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: make_Dictionary\n",
    "\n",
    "This function processes a collection of emails to create a list of the 3000 most common words, excluding non-alphabetical words and single characters, which serves as the feature set for spam email classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjKF0nIMwz8_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the function make_Dictionary with a single parameter root_dir.\n",
    "# This parameter is expected to be a directory path where the email files are stored.\n",
    "\n",
    "def make_Dictionary(root_dir):\n",
    "    \n",
    "    # Initialize a list to store all words from the emails\n",
    "  all_words = []\n",
    "\n",
    "    # Create a list of file paths for all emails in the root directory\n",
    "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
    "\n",
    "    # Loop through each email and extract words\n",
    "  for mail in emails:\n",
    "    with open(mail) as m:\n",
    "      for line in m:\n",
    "        words = line.split()\n",
    "        all_words += words\n",
    "        \n",
    "    # Count the frequency of each word\n",
    "  dictionary = Counter(all_words)\n",
    "\n",
    "    # Create a list to remove unwanted words\n",
    "  list_to_remove = list(dictionary)\n",
    "\n",
    "    # Loop through each item and remove unwanted words\n",
    "  for item in list_to_remove:\n",
    "    if item.isalpha() == False: # checks if the word contains non-alphabetical characters\n",
    "      del dictionary[item]\n",
    "    elif len(item) == 1: # checks if the word is a single character\n",
    "      del dictionary[item]\n",
    "    \n",
    "    # Keep only the 3000 most common words\n",
    "  dictionary = dictionary.most_common(3000)\n",
    "  return dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: extract_features\n",
    "\n",
    "This function reads emails from the specified directory and extracts features for use in a spam classifier. It creates a feature matrix based on the frequency of the most common words (as per the provided dictionary) in each email. It also assigns labels to the emails, marking them as spam or not spam based on their filenames. The feature matrix and the labels are used for training and evaluating the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmVW5xNlyOFc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(mail_dir):\n",
    "    \n",
    "    # Creating a list of file paths for all the emails in the specified directory\n",
    "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "\n",
    "    # Initializing a feature matrix with dimensions: number of files x 3000 (number of words in the dictionary)\n",
    "    # Each row represents an email, and each column represents a word's frequency\n",
    "  features_matrix = np.zeros((len(files),3000))\n",
    "\n",
    "    # Initializing an array to store labels (spam or not spam) for each email\n",
    "  train_labels = np.zeros(len(files))\n",
    "\n",
    "    # Initializing counters\n",
    "  count = 1;\n",
    "  docID = 0;\n",
    "    \n",
    "    \n",
    "    # Iterating over each email file\n",
    "  for fil in files:\n",
    "    with open(fil) as fi:\n",
    "        \n",
    "        # Reading each line in the email file\n",
    "      for i, line in enumerate(fi):\n",
    "        if i ==2: # Processing the content from the third line\n",
    "          words = line.split() # Splitting the line into words\n",
    "        \n",
    "            # Iterating over each word in the line\n",
    "          for word in words:\n",
    "            wordID = 0\n",
    "            \n",
    "              # Searching for the word in the dictionary to find its index\n",
    "            for i, d in enumerate(dictionary):\n",
    "                # If word is found in the dictionary, update the feature matrix\n",
    "              if d[0] == word:\n",
    "                wordID = i\n",
    "                features_matrix[docID,wordID] = words.count(word)\n",
    "\n",
    "                \n",
    "        # Default label is 0 (not spam)\n",
    "      train_labels[docID] = 0;\n",
    "        # Splitting the file path to extract the file name\n",
    "      filepathTokens = fil.split('/')\n",
    "        # Extracting the last token which is the file name\n",
    "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
    " \n",
    "        # If filename starts with \"spmsg\", it's a spam email, label is set to 1\n",
    "      if lastToken.startswith(\"spmsg\"):\n",
    "        train_labels[docID] = 1;\n",
    "        count = count + 1\n",
    "        \n",
    "        # Incrementing docID for the next email\n",
    "      docID = docID + 1\n",
    "    \n",
    "    # Returning the feature matrix and labels for the emails\n",
    "  return features_matrix, train_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoq-rE7Mx0pp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Enter the \"path\" of your \"train_mails\" and \"test-mails\" FOLDERS in this cell ...\n",
    "TRAIN_DIR = './train-mails'\n",
    "TEST_DIR = './test-mails'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing emails from TRAIN and TEST folders\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary from the training emails\n",
    "dictionary = make_Dictionary(TRAIN_DIR)\n",
    "\n",
    "# Extracting features and labels from both training and test datasets\n",
    "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
    "features_matrix, labels = extract_features(TRAIN_DIR)\n",
    "test_features_matrix, test_labels = extract_features(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing emails from TRAIN and TEST folders\n",
      "Training Model using Gaussian Naive Bayes algorithm .....\n",
      "Training completed\n",
      "testing trained model to predict Test Data labels\n",
      "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
      "0.9653846153846154\n"
     ]
    }
   ],
   "source": [
    "# In this section enter your code to TRAIN the model using Naive Bayes algorithm, then PREDICT and then evaluate PERFORMANCE (Accuracy)\n",
    "\n",
    "print(\"reading and processing emails from TRAIN and TEST folders\")\n",
    "print(\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(features_matrix, labels)\n",
    "print(\"Training completed\")\n",
    "\n",
    "print(\"testing trained model to predict Test Data labels\")\n",
    "# Predict the labels for the test data\n",
    "predicted_labels = model.predict(test_features_matrix)\n",
    "print(\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
    "\n",
    "# Evaluate the performance using accuracy\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5_mPrvN586A"
   },
   "source": [
    "======================= END OF PROGRAM ========================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOaSi3qlFUlqTup/1esXCKD",
   "collapsed_sections": [],
   "name": "naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
